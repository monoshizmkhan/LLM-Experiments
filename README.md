# LLM-Experiments

This is a toy project to train an LLM, implement a RAG, and pass the LLM a prompt that it will use the RAG to answer.

To fine-tune the pre-trained GPT2 model, run LLM_train.py.

Due to resource constraints, the number of epochs is set to 1.

To load this trained GPT2 model, and use the RAG for prompts, run RAG.py.

The variables in both files can be edited as needed.


Planned future parts of this project include using LangChain modules.
